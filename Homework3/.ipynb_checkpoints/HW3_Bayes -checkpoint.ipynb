{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (20% credit)\n",
    "\n",
    "Assume you have a data set as below. It contains records of cars with three features: the type of the car (sports(1) or SUV(2)), the color of the car (red(1) or yellow(2)), and the origin of the car (domestic(1) or imported(2)). And the labels for the data are: stolen(1) and not(0). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "y=[1,0,1,0,1,0,1,0,0,1]\n",
    "X=[[1,1,1,2,2,2,2,2,1,1],[1,1,1,1,1,2,2,2,2,1],[1,1,1,1,2,2,2,1,2,2]]\n",
    "data=[y]+X\n",
    "data=pd.DataFrame(data).T\n",
    "data.columns=['Stolen?','Color','Type','Origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stolen?</th>\n",
       "      <th>Color</th>\n",
       "      <th>Type</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stolen?  Color  Type  Origin\n",
       "0        1      1     1       1\n",
       "1        0      1     1       1\n",
       "2        1      1     1       1\n",
       "3        0      2     1       1\n",
       "4        1      2     1       2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (A)\n",
    "\n",
    "Calculate the following sample probabilities:\n",
    "P(Red|Stolen), P(SUV|Stolen), P(Domestic|Stolen), P(Red|Not Stolen) , P(SUV|Not Stolen), and P(Domestic|Not Stolen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Red|Stolen): 0.6\n",
      "P(SUV|Stolen): 0.4\n",
      "P(Domestic|Stolen): 0.4\n",
      "P(Red|Not Stolen): 0.4\n",
      "P(SUV|Not Stolen): 0.6\n",
      "P(Domestic|Not Stolen): 0.6\n"
     ]
    }
   ],
   "source": [
    "print 'P(Red|Stolen):',float(3/5)\n",
    "\n",
    "print 'P(SUV|Stolen):', float(2/5)\n",
    "\n",
    "print 'P(Domestic|Stolen):', float(2/5)\n",
    "\n",
    "print 'P(Red|Not Stolen):', float(2/5)\n",
    "\n",
    "print 'P(SUV|Not Stolen):', float(3/5)\n",
    "\n",
    "print 'P(Domestic|Not Stolen):', float(3/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (B)\n",
    "\n",
    "Suggest a classification for a red, domestic SUV - whether it will be stolen or not - using Naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(Stolen|\\{Red,Domestic\\})\\sim P(Red|Stolen)P(Domestic|Stolen)P(!SUV|Stolen)P(Stolen)=3/5*2/5*(1-3/5)*1/2=0.048\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P(Not Stolen|\\{Red,Domestic\\})\\sim P(Red|Not Stolen)P(Domestic|Not Stolen)P(!SUV|Non Stolen)P(Non Stolen)=2/5*3/5*(1-2/5)*1/2=0.072\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Stolen|\\{Red,Domestic\\}) < P(Not Stolen|\\{Red,Domestic\\}) = Not Stolen\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (25% credit)\n",
    "Consider a following Guassian Naive Bayes problem.\n",
    "We use eight factors to predict if people have diabetes or not. The variabls are:\n",
    "\n",
    "y: The label (0 - no diabetes, 1 - diabetes)\n",
    "\n",
    "t_pre: Number of times pregnant\n",
    "\n",
    "glu: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "\n",
    "blood_p: Diastolic blood pressure (mm Hg)\n",
    "\n",
    "triceps: Triceps skin fold thickness (mm)\n",
    "\n",
    "serum: 2-Hour serum insulin (mu U/ml)\n",
    "\n",
    "b_m: Body mass index (weight in kg/(height in m)^2)\n",
    "\n",
    "pedigree_f: Diabetes pedigree function\n",
    "\n",
    "age: Age (years)\n",
    "#### ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Questions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "\n",
    "Train the classifier: use the training data to estimate prior probabilities $P(y=b)$ as well as the parameters (mean and standard deviation) of the sample distributions $P(x_i|y=b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"dia_train.csv\", usecols=range(1,10)) \n",
    "#y_train=data_train.iloc[:,1] \n",
    "#X_train=data_train.iloc[:,2:] \n",
    "\n",
    "def trainNaiveBayes(trainData):\n",
    "  #training Gausian Naive Bayes Classifier\n",
    "  tY=trainData.loc[:,trainData.columns[0]]\n",
    "  ind1=tY==0\n",
    "  ind2=tY==1\n",
    "  dp=pd.DataFrame(columns=trainData.columns, index=['mu1','sigma1','mu2','sigma2'])\n",
    "  #estimate priors\n",
    "  dp[trainData.columns[0]]['mu1']=1.0*sum(ind1)/len(trainData.index)\n",
    "  dp[trainData.columns[0]]['mu2']=1.0*sum(ind2)/len(trainData.index)\n",
    "  #estimate sample distribution paramters for p(xi|y=b)\n",
    "  for i in trainData.columns[1:]:\n",
    "    dp.loc['mu1',i]=(trainData[i][ind1]).mean()\n",
    "    dp.loc['sigma1',i]=(trainData[i][ind1]).std()\n",
    "    dp.loc['mu2',i]=(trainData[i][ind2]).mean()\n",
    "    dp.loc['sigma2',i]=(trainData[i][ind2]).std()\n",
    "  return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>t_pre</th>\n",
       "      <th>glu</th>\n",
       "      <th>blood_p</th>\n",
       "      <th>triceps</th>\n",
       "      <th>serum</th>\n",
       "      <th>b_m</th>\n",
       "      <th>pedigree_f</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>158</td>\n",
       "      <td>64</td>\n",
       "      <td>13</td>\n",
       "      <td>387</td>\n",
       "      <td>31.2</td>\n",
       "      <td>0.295</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>64</td>\n",
       "      <td>22</td>\n",
       "      <td>66</td>\n",
       "      <td>35.8</td>\n",
       "      <td>0.545</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>120</td>\n",
       "      <td>72</td>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0.733</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>76</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.118</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>66</td>\n",
       "      <td>20</td>\n",
       "      <td>90</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.867</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y  t_pre  glu  blood_p  triceps  serum   b_m  pedigree_f  age\n",
       "0  0      3  158       64       13    387  31.2       0.295   24\n",
       "1  0      0   84       64       22     66  35.8       0.545   21\n",
       "2  0      9  120       72       22     56  20.8       0.733   48\n",
       "3  0      4  110       76       20    100  28.4       0.118   27\n",
       "4  1      2  100       66       20     90  32.9       0.867   28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNaiveBayes(classData,dp):\n",
    "  #classifying using trained Gausian Naive Bayes Classifier\n",
    "  Y=classData.loc[:,classData.columns[0]]*0\n",
    "  for j in classData.index:\n",
    "    #start from the priors\n",
    "    P1=dp[classData.columns[0]]['mu1'];\n",
    "    P2=dp[classData.columns[0]]['mu2'];\n",
    "    #multiply by conditional probability densities p(xi|y=b)\n",
    "    for i in classData.columns[1:]:\n",
    "        if dp[i]['sigma1']==0: #if sigma can not be defined (sample does not have variance)\n",
    "            P1=P1*stats.norm.pdf(classData[i][j], loc=dp[i]['mu1'],scale=1) #pick up arbitrary sigma if undefined\n",
    "        else:\n",
    "            P1=P1*stats.norm.pdf(classData[i][j], loc=dp[i]['mu1'],scale=dp[i]['sigma1'])\n",
    "        \n",
    "        if dp[i]['sigma2']==0: #if sigma can not be defined (sample does not have variance)\n",
    "            P2=P2*stats.norm.pdf(classData[i][j], loc=dp[i]['mu2'],scale=1) #pick up arbitrary sigma if undefined\n",
    "        else:\n",
    "            P2=P2*stats.norm.pdf(classData[i][j], loc=dp[i]['mu2'],scale=dp[i]['sigma2']) \n",
    "    Y[j]=int(P2>P1)\n",
    " \n",
    "\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now train the classifier based on the training sample\n",
    "dp=trainNaiveBayes(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>t_pre</th>\n",
       "      <th>glu</th>\n",
       "      <th>blood_p</th>\n",
       "      <th>triceps</th>\n",
       "      <th>serum</th>\n",
       "      <th>b_m</th>\n",
       "      <th>pedigree_f</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu1</th>\n",
       "      <td>0.6991525</td>\n",
       "      <td>2.690909</td>\n",
       "      <td>111.4667</td>\n",
       "      <td>69.20606</td>\n",
       "      <td>27.2</td>\n",
       "      <td>127.0061</td>\n",
       "      <td>31.70909</td>\n",
       "      <td>0.4686848</td>\n",
       "      <td>28.39394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.610061</td>\n",
       "      <td>24.69189</td>\n",
       "      <td>11.71329</td>\n",
       "      <td>10.43692</td>\n",
       "      <td>91.48614</td>\n",
       "      <td>6.337613</td>\n",
       "      <td>0.2917503</td>\n",
       "      <td>8.537362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu2</th>\n",
       "      <td>0.3008475</td>\n",
       "      <td>4.070423</td>\n",
       "      <td>144.1408</td>\n",
       "      <td>74.56338</td>\n",
       "      <td>33.47887</td>\n",
       "      <td>209.2113</td>\n",
       "      <td>35.22394</td>\n",
       "      <td>0.6390423</td>\n",
       "      <td>35.78873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.51862</td>\n",
       "      <td>30.62646</td>\n",
       "      <td>13.79931</td>\n",
       "      <td>9.762697</td>\n",
       "      <td>126.921</td>\n",
       "      <td>6.258491</td>\n",
       "      <td>0.439042</td>\n",
       "      <td>10.26355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                y     t_pre       glu   blood_p   triceps     serum       b_m  \\\n",
       "mu1     0.6991525  2.690909  111.4667  69.20606      27.2  127.0061  31.70909   \n",
       "sigma1        NaN  2.610061  24.69189  11.71329  10.43692  91.48614  6.337613   \n",
       "mu2     0.3008475  4.070423  144.1408  74.56338  33.47887  209.2113  35.22394   \n",
       "sigma2        NaN   3.51862  30.62646  13.79931  9.762697   126.921  6.258491   \n",
       "\n",
       "       pedigree_f       age  \n",
       "mu1     0.4686848  28.39394  \n",
       "sigma1  0.2917503  8.537362  \n",
       "mu2     0.6390423  35.78873  \n",
       "sigma2   0.439042  10.26355  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "\n",
    "Perform the classification for the test sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "data_test=pd.read_csv(\"dia_test.csv\", usecols=range(1,10))\n",
    "y_test=data_test.iloc[:,0]\n",
    "X_test=data_test.iloc[:,1:]\n",
    "\n",
    "C=classifyNaiveBayes(data_test,dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C) \n",
    "\n",
    "Compare your result to y_test and report the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy over the test set is: 0.791139240506\n"
     ]
    }
   ],
   "source": [
    "#classification accuracy (over test set)\n",
    "print \"The classification accuracy over the test set is:\", 1.0*sum(C==y_test)/len(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (Credit 25%)\n",
    "We have an artificial data set split, while the training set contains both - labeled (Label_train) and unlabeled (Unlabel) data. Column 'y' is the label, and columns '0','1','2' are categorical variables.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"EM_train.csv\", usecols=range(1,5))\n",
    "y_Label_train=data_train.iloc[:,0] \n",
    "X_Label_train=data_train.iloc[:,1:] \n",
    "\n",
    "data_test=pd.read_csv(\"EM_test.csv\", usecols=range(1,5))\n",
    "y_Label_test=data_test.iloc[:,0]\n",
    "X_Label_test=data_test.iloc[:,1:]\n",
    "\n",
    "data_Unlabel=pd.read_csv(\"EM_Unlabel.csv\", usecols=range(1,4))\n",
    "X_Unlabel=data_Unlabel.iloc[:,0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNaiveBayesDiscrete(trainData):\n",
    "  #training discrete Naive Bayes Classifier\n",
    "  tY=trainData.loc[:,trainData.columns[0]]\n",
    "  m=max([trainData[j][i] for j in trainData.columns[1:] for i in trainData.index]) #maximal number of classes in each feature of a training set\n",
    "  #create output data structure for the probabilities - same column labels, rows correspond to values of x and there are two arrays like that for different b\n",
    "  dp=[pd.DataFrame(columns=trainData.columns, index=range(1,m+1)), pd.DataFrame(columns=trainData.columns, index=range(1,m+1))]\n",
    "  #split the training data between two labels\n",
    "  ind1=tY==0\n",
    "  ind2=tY==1\n",
    "  #estimate P(y=b)  \n",
    "  dp[0][trainData.columns[0]][1]=1.0*ind1.sum()/len(trainData.index)\n",
    "  dp[1][trainData.columns[0]][1]=1.0*ind2.sum()/len(trainData.index)\n",
    "  #estimate conditional probabilities P(x|y=b)\n",
    "  for j in trainData.columns[1:]:\n",
    "    for i in range(1,m+1):\n",
    "        dp[0].loc[i,j]=1.0*(trainData[j][ind1]==i).sum()/ind1.sum();\n",
    "        dp[1].loc[i,j]=1.0*(trainData[j][ind2]==i).sum()/ind2.sum();\n",
    "  return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNaiveBayesDiscrete(classData,dp):\n",
    "  #classifying using trained discrete Naive Bayes Classifier\n",
    "  Y=classData[classData.columns[0]]*0 #initialize the empty array \n",
    "  for i in classData.index: #for al records to classify\n",
    "    #start with the priors\n",
    "    P1=dp[0][classData.columns[0]][1]; \n",
    "    P2=dp[1][classData.columns[0]][1];\n",
    "    #and multiply them by the corresponding conditional probabilities P(x_i|y=b)\n",
    "    for j in classData.columns[1:]:\n",
    "      P1=P1*dp[0][j][classData[j][i]]\n",
    "      P2=P2*dp[1][j][classData[j][i]]\n",
    "    Y[i]=int(P2>P1) #finally for each record decide which P(y|x) is higher and choose the label\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) \n",
    "\n",
    "Use the labeled part data_train to predict the labels of X_Label_test, and report the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[           y    0    1    2\n",
       " 1  0.5555556    0  0.2    0\n",
       " 2        NaN  0.2    0  0.2\n",
       " 3        NaN  0.2    0    0\n",
       " 4        NaN  0.2  0.2  0.2\n",
       " 5        NaN    0    0    0\n",
       " 6        NaN  0.4  0.6  0.6,            y     0     1     2\n",
       " 1  0.4444444   0.5  0.25  0.25\n",
       " 2        NaN  0.25   0.5   0.5\n",
       " 3        NaN     0  0.25     0\n",
       " 4        NaN     0     0     0\n",
       " 5        NaN  0.25     0     0\n",
       " 6        NaN     0     0  0.25]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp=trainNaiveBayesDiscrete(data_train)\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C=classifyNaiveBayesDiscrete(data_test,dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We correctly classified 63.8888888889 percent of the artificial data set based on the labeled data only\n"
     ]
    }
   ],
   "source": [
    "acc=(100.0*sum(C==y_Label_test)/len(y_Label_test))\n",
    "print \"We correctly classified {0} percent of the artificial data set based on the labeled data only\".format(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) \n",
    "\n",
    "Improve the classification by using the unlabeled data data_Unlabel and the EM algorithm to predict labels of X_Label_test, and report the new accuracy by EM semi-supervised algorithm (use the same convergence criteria as in the lecture notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implementation of Expectation-Maximization algorithm for partially labeled data\n",
    "def EM(X_Label,y_Label,X_Unlabel,dp):\n",
    "  t = 0  \n",
    "  haslabels=len(y_Label_test)>0\n",
    "\n",
    "  while True:\n",
    "    t = t + 1\n",
    "\n",
    "    classData=X_Unlabel\n",
    "    # Now we want to calculate P(y=1|x) and P(y=2|x) for all observations xj. (these are bunch of scalars)\n",
    "    # we need this to calculate new dp. Basically speaking, for every new iteration we need a new dp.\n",
    "\n",
    "    #for y=1 and y=2\n",
    "\n",
    "    p_x_1=[] #unnormalized P(y=1|x)\n",
    "    p_x_2=[] #unnormalized P(y=2|x)\n",
    "    cols=dp[0].columns\n",
    "\n",
    "    for i in classData.index:\n",
    "        P1=dp[0][cols[0]][1];\n",
    "        P2=dp[1][cols[0]][1];\n",
    "        for j in classData.columns:\n",
    "            P1=P1*dp[0][j][classData[j][i]]\n",
    "            P2=P2*dp[1][j][classData[j][i]]\n",
    "        p_x_1.append(P1)\n",
    "        p_x_2.append(P2)\n",
    "\n",
    "    #Rescale p_x_1 and p_x_2:\n",
    "    summ=np.asarray(p_x_1)+np.asarray(p_x_2)\n",
    "    p_x_1_s=np.asarray(p_x_1)/summ\n",
    "    p_x_2_s=np.asarray(p_x_2)/summ\n",
    "    inds_1 = np.where(np.isnan(p_x_1_s))\n",
    "    inds_2 = np.where(np.isnan(p_x_2_s))\n",
    "    p_x_1_s[inds_1]=0.5\n",
    "    p_x_2_s[inds_2]=0.5\n",
    "    #Now let's calculate P(y=1) and P(y=2)\n",
    "    p_1=p_x_1_s.sum()/len(p_x_1_s)\n",
    "    p_2=p_x_2_s.sum()/len(p_x_2_s)\n",
    "\n",
    "\n",
    "    #Now let's calculate the probability distribution of P(xi|y=1) and P(xi|y=2)\n",
    "    \n",
    "    m=max([classData[j][i] for j in classData.columns for i in classData.index]) #maximal number of classes in each feature of a training set\n",
    "\n",
    "    #create output data structure for the probabilities - new iteration\n",
    "    \n",
    "    dp1=[pd.DataFrame(columns=cols, index=range(1,m+1)), pd.DataFrame(columns=cols, index=range(1,m+1))]\n",
    "\n",
    "    #P(y=b)  \n",
    "    dp1[0][cols[0]][1]=p_1\n",
    "    dp1[1][cols[0]][1]=p_2\n",
    "\n",
    "\n",
    "    #estimate conditional probabilities P(x|y=b) -do we add labeled data to fit?\n",
    "\n",
    "    temp=np.concatenate((np.asmatrix(X_Unlabel),np.asarray(pd.DataFrame(p_x_1_s)),np.asarray(pd.DataFrame(p_x_2_s))), axis=1)\n",
    "    temp=pd.DataFrame(temp)\n",
    "    if haslabels:\n",
    "        temp_l=np.concatenate((np.asmatrix(X_Label),np.asmatrix(1*(y_Label==0)).transpose(),np.asmatrix(1*(y_Label==1)).transpose()),axis=1)\n",
    "        temp_l=pd.DataFrame(temp_l)\n",
    "        pd.concat([temp,temp_l])\n",
    "   \n",
    "\n",
    "    for j in range(1,len(dp[0].T)):\n",
    "        for i in range(len(dp[0])):\n",
    "\n",
    "            dp1[0].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-2].sum()/temp.iloc[:,-2].sum()\n",
    "            dp1[1].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-1].sum()/temp.iloc[:,-1].sum()\n",
    "\n",
    "        ############################################################################################\n",
    "    # Now we use dp to decide whether to continue our iterations\n",
    "    \n",
    "    if ((sum(np.sum(dp1[0]-dp[0])**2)) + sum(np.sum(dp1[1]-dp[1])**2))<0.001: #if dp does not change much\n",
    "        break\n",
    "    else: \n",
    "        dp=dp1  #save new dp and perform next iteration\n",
    "\n",
    "        \n",
    "    ###############################################################################################\n",
    "        #Calculate the log-likelihood\n",
    "        \n",
    "        L=0\n",
    "        \n",
    "        for i in classData.index:\n",
    "            P1=dp[0][cols[0]][1];\n",
    "            P2=dp[1][cols[0]][1];\n",
    "            for j in classData.columns:\n",
    "                P1=P1*dp[0][j][classData[j][i]]\n",
    "                P2=P2*dp[1][j][classData[j][i]]\n",
    "            temp=math.log(P1+P2)\n",
    "            L=L+temp\n",
    "        if haslabels:    \n",
    "          for i in X_Label.index:\n",
    "            yi=y_Label[i]\n",
    "            P=dp[yi][cols[0]][1];\n",
    "            for j in X_Label.columns:\n",
    "                P=P*dp[yi][j][X_Label[j][i]]\n",
    "            L=L+math.log(P)\n",
    "        \n",
    "        print \"Iteration {0}: log maximum liklihood = {1}\".format(t,L)    \n",
    "        \n",
    "        \n",
    "  return dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: log maximum liklihood = -744.261718032\n",
      "After EM we correctly classified 91.6666666667 percents of the trips\n"
     ]
    }
   ],
   "source": [
    "#perform EM estimation for theta\n",
    "dpEM=EM(X_Label_test,y_Label_test,X_Unlabel,dp)\n",
    "#OS test\n",
    "C=classifyNaiveBayesDiscrete(data_test,dpEM) #classify test data with a new theta given by EM\n",
    "acc=100.0*sum(C==y_Label_test)/len(y_Label_test)\n",
    "print \"After EM we correctly classified {0} percents of the trips\".format(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (Credit 30%)\n",
    "For the similar artifitial data uploaded below:\n",
    "\n",
    "#### Question: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"EM_Cluster.csv\")\n",
    "X=data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  5  6  6\n",
       "1  4  6  6\n",
       "2  1  5  5\n",
       "3  1  3  4\n",
       "4  5  6  2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2016)\n",
    "#initialize theta randomly\n",
    "dp[0].iloc[0,0]=np.random.uniform(0,1)\n",
    "dp[1].iloc[0,0]=dp[1].iloc[0,0]=1-dp[0].iloc[0,0]\n",
    "for j in range(1,len(dp[0].T)):\n",
    "    b=np.random.uniform(0,1,len(dp[0]))\n",
    "    b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "    dp[0].iloc[:,j]=b\n",
    "for j in range(1,len(dp[1].T)):\n",
    "    b=np.random.uniform(0,1,len(dp[1]))\n",
    "    b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "    dp[1].iloc[:,j]=b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[           y         0         1         2\n",
       " 1  0.8967054  0.203711  0.233433  0.052900\n",
       " 2        NaN  0.218506  0.246143  0.221832\n",
       " 3        NaN  0.206894  0.205944  0.216376\n",
       " 4        NaN  0.128907  0.191068  0.025485\n",
       " 5        NaN  0.179253  0.079939  0.346481\n",
       " 6        NaN  0.062729  0.043473  0.136927,\n",
       "            y         0         1         2\n",
       " 1  0.1032946  0.275895  0.145768  0.248003\n",
       " 2        NaN  0.275194  0.118057  0.187649\n",
       " 3        NaN  0.020212  0.159788  0.189117\n",
       " 4        NaN  0.103416  0.346520  0.101663\n",
       " 5        NaN  0.109967  0.161480  0.147634\n",
       " 6        NaN  0.215316  0.068387  0.125933]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#implementation of Expectation-Maximization algorithm for partially labeled data\n",
    "def EM2(X_Label,y_Label,X_Unlabel,dp):\n",
    "  t = 0  \n",
    "  haslabels=len(y_Label_test)>0\n",
    "\n",
    "  while True:\n",
    "    t = t + 1\n",
    "\n",
    "    classData=X_Unlabel\n",
    "    # Now we want to calculate P(y=1|x) and P(y=2|x) for all observations xj. (these are bunch of scalars)\n",
    "    # we need this to calculate new dp. Basically speaking, for every new iteration we need a new dp.\n",
    "\n",
    "    #for y=1 and y=2\n",
    "\n",
    "    p_x_1=[] #unnormalized P(y=1|x)\n",
    "    p_x_2=[] #unnormalized P(y=2|x)\n",
    "    cols=dp[0].columns\n",
    "\n",
    "    for i in classData.index:\n",
    "        P1=dp[0][cols[0]][1];\n",
    "        P2=dp[1][cols[0]][1];\n",
    "        for j in classData.columns:\n",
    "            P1=P1*dp[0][j][classData[j][i]]\n",
    "            P2=P2*dp[1][j][classData[j][i]]\n",
    "        p_x_1.append(P1)\n",
    "        p_x_2.append(P2)\n",
    "\n",
    "    #Rescale p_x_1 and p_x_2:\n",
    "    summ=np.asarray(p_x_1)+np.asarray(p_x_2)\n",
    "    p_x_1_s=np.asarray(p_x_1)/summ\n",
    "    p_x_2_s=np.asarray(p_x_2)/summ\n",
    "    inds_1 = np.where(np.isnan(p_x_1_s))\n",
    "    inds_2 = np.where(np.isnan(p_x_2_s))\n",
    "    p_x_1_s[inds_1]=0.5\n",
    "    p_x_2_s[inds_2]=0.5\n",
    "    #Now let's calculate P(y=1) and P(y=2)\n",
    "    p_1=p_x_1_s.sum()/len(p_x_1_s)\n",
    "    p_2=p_x_2_s.sum()/len(p_x_2_s)\n",
    "\n",
    "\n",
    "    #Now let's calculate the probability distribution of P(xi|y=1) and P(xi|y=2)\n",
    "    \n",
    "    m=max([classData[j][i] for j in classData.columns for i in classData.index]) #maximal number of classes in each feature of a training set\n",
    "\n",
    "    #create output data structure for the probabilities - new iteration\n",
    "    \n",
    "    dp1=[pd.DataFrame(columns=cols, index=range(1,m+1)), pd.DataFrame(columns=cols, index=range(1,m+1))]\n",
    "\n",
    "    #P(y=b)  \n",
    "    dp1[0][cols[0]][1]=p_1\n",
    "    dp1[1][cols[0]][1]=p_2\n",
    "\n",
    "\n",
    "    #estimate conditional probabilities P(x|y=b) -do we add labeled data to fit?\n",
    "\n",
    "    temp=np.concatenate((np.asmatrix(X_Unlabel),np.asarray(pd.DataFrame(p_x_1_s)),np.asarray(pd.DataFrame(p_x_2_s))), axis=1)\n",
    "    temp=pd.DataFrame(temp)\n",
    "    if haslabels:\n",
    "        temp_l=np.concatenate((np.asmatrix(X_Label),np.asmatrix(1*(y_Label==0)).transpose(),np.asmatrix(1*(y_Label==1)).transpose()),axis=1)\n",
    "        temp_l=pd.DataFrame(temp_l)\n",
    "        pd.concat([temp,temp_l])\n",
    "   \n",
    "\n",
    "    for j in range(1,len(dp[0].T)):\n",
    "        for i in range(len(dp[0])):\n",
    "\n",
    "            dp1[0].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-2].sum()/temp.iloc[:,-2].sum()\n",
    "            dp1[1].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-1].sum()/temp.iloc[:,-1].sum()\n",
    "\n",
    "        ############################################################################################\n",
    "    # Now we use dp to decide whether to continue our iterations\n",
    "    \n",
    "    if ((sum(np.sum(dp1[0]-dp[0])**2)) + sum(np.sum(dp1[1]-dp[1])**2))<0.001: #if dp does not change much\n",
    "        break\n",
    "    else: \n",
    "        dp=dp1  #save new dp and perform next iteration\n",
    "\n",
    "        \n",
    "    ###############################################################################################\n",
    "        #Calculate the log-likelihood\n",
    "        \n",
    "        L=0\n",
    "        \n",
    "        for i in classData.index:\n",
    "            P1=dp[0][cols[0]][1];\n",
    "            P2=dp[1][cols[0]][1];\n",
    "            for j in classData.columns:\n",
    "                P1=P1*dp[0][j][classData[j][i]]\n",
    "                P2=P2*dp[1][j][classData[j][i]]\n",
    "            temp=math.log(P1+P2)\n",
    "            L=L+temp\n",
    "        #if haslabels:    \n",
    "         # for i in X_Label.index:\n",
    "        #  yi=y_Label[i]\n",
    "        #     P=dp[yi][cols[0]][1];\n",
    "         #   for j in X_Label.columns:\n",
    "        #      P=P*dp[yi][j][X_Label[j][i]]\n",
    "           # 3L=L+math.log(P)\n",
    "        \n",
    "        print \"Iteration {0}: log maximum liklihood = {1}\".format(t,L)    \n",
    "        \n",
    "        \n",
    "  return dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: log maximum liklihood = -550.945248537\n",
      "[           y           0          1           2\n",
      "1  0.8261141   0.2225422   0.154627   0.1671593\n",
      "2        NaN  0.09910279  0.2622571   0.2167209\n",
      "3        NaN   0.1703821  0.1170647   0.1504559\n",
      "4        NaN   0.2322533  0.1461682  0.07273329\n",
      "5        NaN   0.1964081  0.1359577   0.2219638\n",
      "6        NaN  0.07931145  0.1839254   0.1709668,            y           0           1           2\n",
      "1  0.1738859   0.2572156  0.08693984   0.3012523\n",
      "2        NaN   0.1864182   0.1233049  0.06579016\n",
      "3        NaN  0.01208871   0.1010832  0.05198585\n",
      "4        NaN   0.1563089   0.1271265    0.311697\n",
      "5        NaN   0.1075238   0.2851768  0.09565213\n",
      "6        NaN   0.2804448   0.2763688   0.1736226]\n"
     ]
    }
   ],
   "source": [
    "#perform EM estimation for theta\n",
    "dpEM=EM2([],[],X,dp)\n",
    "#OS test\n",
    "C=classifyNaiveBayesDiscrete(data_test,dpEM) #classify test data with a new theta given by EM\n",
    "print dpEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) \n",
    "\n",
    "Apply the EM algorithm (no observed labels, random initial choice of $\\theta$) for clustering the data records into two clusters. Report your result (a vector of cluster numbers for each data record). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    0\n",
       "19    0\n",
       "20    0\n",
       "21    0\n",
       "22    0\n",
       "23    0\n",
       "24    0\n",
       "25    0\n",
       "26    0\n",
       "27    0\n",
       "28    0\n",
       "29    0\n",
       "30    0\n",
       "31    0\n",
       "32    0\n",
       "33    1\n",
       "34    1\n",
       "35    0\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) \n",
    "\n",
    "Repeat the clustering 10 times with different random choices of $\\theta$ and analyze the stability of the clustering (matching labels accross different clusterings (use the choice of 0 and 1 labels best matching the previous clustering), estimate average label and its standard error for each record)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: log maximum liklihood = -534.408980849\n",
      "Iteration 1: log maximum liklihood = -549.039120484\n",
      "Iteration 1: log maximum liklihood = -556.261873821\n",
      "Iteration 1: log maximum liklihood = -552.530038876\n",
      "Iteration 1: log maximum liklihood = -546.241774499\n",
      "Iteration 1: log maximum liklihood = -550.025433635\n",
      "Iteration 1: log maximum liklihood = -553.888122335\n",
      "Iteration 1: log maximum liklihood = -553.888164557\n",
      "Iteration 1: log maximum liklihood = -556.0418992\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2015)\n",
    "#initialize theta randomly\n",
    "\n",
    "test = []\n",
    "values = []\n",
    "for q in range(10):\n",
    "    dp[0].iloc[0,0]=np.random.uniform(0,1)\n",
    "    dp[1].iloc[0,0]=dp[1].iloc[0,0]=1-dp[0].iloc[0,0]\n",
    "    for j in range(1,len(dp[0].T)):\n",
    "        b=np.random.uniform(0,1,len(dp[0]))\n",
    "        b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "        dp[0].iloc[:,j]=b\n",
    "        test.append(dp[0].iloc[:,j])\n",
    "    for j in range(1,len(dp[1].T)):\n",
    "        b=np.random.uniform(0,1,len(dp[1]))\n",
    "        b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "        dp[1].iloc[:,j]=b\n",
    "        test.append(dp[1].iloc[:,j])\n",
    "        #perform EM estimation for theta\n",
    "    dpEM=EM2([],[],X,dp)\n",
    "    C=classifyNaiveBayesDiscrete(data_test,dpEM) #classify test data with a new theta given by EM\n",
    "    values.append(C)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9  ...  26  27  28  29  30  31  32  33  \\\n",
       "y   1   1   0   1   0   0   0   0   0   0 ...   1   0   1   0   0   0   1   0   \n",
       "y   1   0   1   0   0   1   1   1   1   0 ...   0   1   0   1   0   1   1   0   \n",
       "y   0   0   0   0   0   0   0   0   0   0 ...   0   0   1   0   0   0   0   1   \n",
       "y   1   1   1   1   0   1   1   1   1   0 ...   0   1   1   1   0   1   1   1   \n",
       "y   1   0   0   0   0   0   0   0   0   0 ...   1   0   1   0   0   0   0   0   \n",
       "y   0   0   0   0   1   0   0   0   1   1 ...   0   0   0   0   1   1   0   1   \n",
       "y   0   0   0   0   0   1   1   0   0   0 ...   0   0   0   1   0   0   0   0   \n",
       "y   1   1   0   1   1   1   1   0   1   1 ...   1   0   0   1   1   1   1   1   \n",
       "y   0   0   1   0   1   0   0   1   1   1 ...   0   1   0   0   1   1   1   1   \n",
       "y   0   0   1   0   0   1   1   1   0   0 ...   0   1   0   1   0   0   0   0   \n",
       "\n",
       "   34  35  \n",
       "y   1   1  \n",
       "y   0   0  \n",
       "y   1   1  \n",
       "y   1   1  \n",
       "y   0   0  \n",
       "y   0   0  \n",
       "y   0   0  \n",
       "y   1   1  \n",
       "y   0   0  \n",
       "y   0   0  \n",
       "\n",
       "[10 rows x 36 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the columns are the observations and the rows are the different iterations with a new theta\n",
    "#the values are the different classifications \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.5\n",
       "1     0.3\n",
       "2     0.4\n",
       "3     0.3\n",
       "4     0.3\n",
       "5     0.5\n",
       "6     0.5\n",
       "7     0.4\n",
       "8     0.5\n",
       "9     0.3\n",
       "10    0.3\n",
       "11    0.4\n",
       "12    0.4\n",
       "13    0.5\n",
       "14    0.0\n",
       "15    0.5\n",
       "16    0.4\n",
       "17    0.3\n",
       "18    0.4\n",
       "19    0.3\n",
       "20    0.3\n",
       "21    0.4\n",
       "22    0.3\n",
       "23    0.5\n",
       "24    0.5\n",
       "25    0.3\n",
       "26    0.3\n",
       "27    0.4\n",
       "28    0.4\n",
       "29    0.5\n",
       "30    0.3\n",
       "31    0.5\n",
       "32    0.5\n",
       "33    0.5\n",
       "34    0.4\n",
       "35    0.4\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the average label value\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16666667,  0.15275252,  0.16329932,  0.15275252,  0.15275252,\n",
       "        0.16666667,  0.16666667,  0.16329932,  0.16666667,  0.15275252,\n",
       "        0.15275252,  0.16329932,  0.16329932,  0.16666667,  0.        ,\n",
       "        0.16666667,  0.16329932,  0.15275252,  0.16329932,  0.15275252,\n",
       "        0.15275252,  0.16329932,  0.15275252,  0.16666667,  0.16666667,\n",
       "        0.15275252,  0.15275252,  0.16329932,  0.16329932,  0.16666667,\n",
       "        0.15275252,  0.16666667,  0.16666667,  0.16666667,  0.16329932,\n",
       "        0.16329932])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the standard error for each value\n",
    "stats.sem(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
